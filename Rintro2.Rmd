---
title: "(Fast) Introduction to R - Class 2"
author: "Joana Cima"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  pdf_document:
    keep_tex: yes
header-includes:
  - "\\usepackage{hyperref}"
  - "\\hypersetup{colorlinks=true, linkcolor=red, urlcolor=blue}"
subtitle: "Jump into a notebook"
---

# My beamer

> BlaBlaBla

<!-- ## Setup -->

```{r setup, echo=FALSE, include=FALSE}
library(knitr) #to produce dynamic reports, allowing the embedding of R code, its results, and text into a single document
# knitr::opts_chunk$set(echo = TRUE)
library(haven) #to import and export data in SPSS, Stata, and SAS formats, enhancing interoperability with other statistical software
library(rmarkdown) #is used to convert files that contain text and R code into documents of various formats, such as HTML, PDF, and Word
library(writexl) #to export tabular data from R to Excel files
library(Hmisc) #provides a collection of functions for data manipulation, statistical analysis, and graph creation, facilitating various stages of the analytical process
library(naniar) #exploring and visualizing missing data in the dataset
library(here)


# Define working directory
rm(list = ls())
here()
getwd()




```

<!-- ## Load your libraries -->

```{r Libraries & Data, echo=FALSE,results='hide',message=FALSE , include=FALSE}
library(tidyverse) #provides a set of tools for data manipulation, visualization, and modeling
library(readxl) # allows us to read Excel files (.xls or .xlsx) directly into R. 
library(visdat) #to explore our dataset through visualizations.
library(stargazer) #to produce beautiful LaTeX or HTML tables and descriptive statistics from R statistical output
library(GGally) #to extend ggplot2 functionality to create a scatterplot matrix
library(car)
library(lmtest)
library(sandwich)
library(margins)
library(caret)
library(randomForest)
```
# Outline

1. Motivation
2. Data
3. Conceptual discussion


# 3. Import data (from an excel file)


```{r }
nlswork <- as.data.frame(read_excel("nlswork.xlsx"))

```

# 3.1 Drop missing values

```{r }
nlswork_no_na <- drop_na(nlswork)

```

# 4. Descriptive statistics

(...)


# 5. Regression analysis

## 5.1 Regression analysis: OLS

```{r include=FALSE}
#Select a subset of variables 
  db_ols<- nlswork_no_na %>% 
 select(age, ln_wage, collgrad, union, hours)

```

      
```{r include=FALSE}
# Model 1
    ols1 <- lm(data = db_ols, ln_wage ~ age)
        summary(ols1)

```


```{r include=FALSE}
# Model 2
    ols2 <- lm(data = db_ols, ln_wage ~ .)

#Alternative ols2<-lm(data = db_ols, ln_wage ~ age + collgrad + union+ hours)
    

      summary(ols2)

```      
      
### 5.1.1. Variable Selection

Selecting appropriate variables for our model is critical to derive accurate and meaningful results. 

```{r , echo=FALSE, warning=FALSE, message=FALSE}
library(caret)
importance <- varImp(ols2, scale = TRUE)
print(importance)

```

```{r , echo=FALSE, warning=FALSE, message=FALSE}

stepwise.model <- step(ols2, direction = "backward")

summary(stepwise.model)
```

### 5.1.2. Our regression Table

Exemplo de texto.

```{r Regressions, echo=FALSE, warning=FALSE, results='asis'}
      stargazer(ols1,ols2,title = "Regression analysis", 
                model.numbers = FALSE,
                column.labels = c("Model (1)","Model (2)"),
                label = "regressions",
                table.placement = "!ht",
                notes.append = TRUE,
                notes.align="l",
                notes="Standard errors in parentheses.",
                header = FALSE,
                no.space = TRUE,
                #covariate.labels = c("Grade","Experience","Experienced sqrd."),
                omit = c("Constant"),
                omit.stat = c("adj.rsq","f","ser"),
                digits = 3,
                digits.extra = 6,
                omit.yes.no = c("Constant",""),
                dep.var.caption="",
                dep.var.labels.include = FALSE,
                type = "latex",
                style = "qje"
                )

```


### 5.1.3. HYPOTHESIS TESTING: automatic command

Next, we will be testing specific hypotheses about the coefficients in our regression model.

1. **Testing if the Coefficient for `age` is Zero:**  
   We aim to test the null hypothesis that:
   \begin{equation}
   H_0: \beta_{\text{age}} = 0
   \end{equation}
   This tests if `age` has any influence on the dependent variable, after adjusting for other variables in the model.

2. **Testing if the Coefficients for `union` and `collgrad` are Equal:**  
   We will test the null hypothesis that:
   \begin{equation}
   H_0: \beta_{\text{union}} = \beta_{\text{collgrad}}
   \end{equation}
   This checks if the effect of being in a union on the dependent variable is the same as the effect of being a college graduate, when other factors are held constant.


```{r, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}
    
      #linearHypothesis(ols2,c("age=0"))

      linearHypothesis(ols2, "union = collgrad")
      
      #test if union is equal to collgrad and if they are equal to zero
      
      #linearHypothesis(ols2, c("collgrad = 0", "union = 0"))
```

### 5.1.4. Additional quality measures: AIC & BIC

In both the AIC and BIC criteria, the model with the lower values is favored as it suggests a better balance between model fit and model complexity.

```{r, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}
    
      AIC(ols1)
      BIC(ols1)
    
      AIC(ols2)
      BIC(ols2)

```


### 5.1.5. COLINEARITY: VIF

The Variance Inflation Factor (VIF) assesses the severity of multicollinearity in a regression, with values greater than 10 suggesting high correlation between predictors (Wooldrige; Verbeek).

```{r, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}
    
      car::vif(ols2)

```


### 5.1.6. HETEROSKEDASTICITY

Homoscedasticity is a fundamental assumption underlying standard linear regression models, stipulating that the variance of the residuals remains constant across levels of the independent variables. This property ensures that the ordinary least squares (OLS) estimator remains the best linear unbiased estimator (BLUE), providing minimum variance. Violations of homoscedasticity, known as heteroscedasticity, can lead to inefficient and potentially biased coefficient estimates, as well as unreliable standard errors. To rigorously assess the presence of homoscedasticity, researchers often employ diagnostic tests, such as the Breusch-Pagan test.

#### Graphical analysis

```{r, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}
      
plot(ols2$fitted.values, resid(ols2))
abline(h=0, col="red")

```


#### Breusch-Pagan test

```{r, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}
          bptest(ols2)

```

The results from the Breusch-Pagan test for the `ols2` model suggest the presence of heteroscedasticity. The test statistic is \( BP = 199.54 \) with a degree of freedom (df) of 4. The low p-value, effectively zero at \( p < 2.2e-16 \), leads us to reject the null hypothesis of homoscedasticity. 


### 5.1.7. Robust estimation


```{r}

robust_ols2 <- coeftest(ols2, vcov. = vcovHC(ols2, type="HC1"))
print(robust_ols2)
```





```{r Specification issues, echo=FALSE, warning=FALSE, results='asis'}

stargazer(ols2, robust_ols2,
          se = list(NULL, sqrt(diag(vcovHC(ols2, type = "HC1")))),
          title = "Comparison of OLS and Robust Regression Models",
          header = FALSE, 
          model.names = FALSE,
         # robust = TRUE,
          column.labels = c("OLS", "Robust OLS"),
          intercept.bottom = FALSE,
       #   intercept.top = TRUE,
          type = "latex",
          style = "qje")

```


### 5.1.8. Coefficient interpretation with a full model (Model 2)


Given that our dependent variable is the natural logarithm of the salary, the coefficients should be interpreted accordingly. For every additional year in age, the salary is expected to increase by approximately `r (exp(0.007) - 1) * 100`%, holding all other factors constant. Being a college graduate is associated with an estimated increase of about `r (exp(0.377) - 1) * 100`% in the salary, compared to not being a college graduate. Being a member of a union is associated with an approximate `r (exp(0.212) - 1) * 100`% increase in salary, holding everything else constant. All variables are significant at the 1% level.


## 5.2. Binary choice models

When the dependent variable is binary, ordinary least squares regression is not suitable due to the non-continuous nature of the outcome variable. Models such as Logit and Probit are specifically designed to handle such binary outcomes. In our analysis, where we aim to understand the probability of a worker being unionized, these models are appropriate choices as they provide insights into the factors influencing this binary decision.These models show the direction of the relationship between independent variables and the dependent variable but don't quantify the magnitude.

```{r echo=FALSE}
# Probit
probit <- glm(union ~ ., data = db_ols, family = binomial(link = "probit"))
summary(probit)

```


```{r echo=FALSE, include=FALSE}
# Logit
logit <- glm(union ~ ., data=db_ols, family = binomial(link = "logit"))
summary(logit)

```

```{r, echo=FALSE, warning=FALSE, results='asis'}
#export results 2 models

stargazer(logit, probit, type = "latex",
          title = "Comparative Table of Logit and Probit Models",
          style = "qje")
```

### 5.2.1. Marginal effects

Marginal effects are crucial in non-linear models like Logit and Probit. While the model's coefficients tell us about the direction of effects, they don't show the actual change in probability for a one-unit change in the predictor. Marginal effects provide this information, making it easier to understand the real-world impact of each variable on the outcome.


#### 5.2.2. Marginal effects - probit

```{r echo=FALSE, include=TRUE}
m2 <- margins(probit)
summary(m2)

```

#### 5.2.3. Marginal effects - logit

```{r echo=FALSE, include=TRUE}
m3 <- margins(logit)
summary(m3)
```

#### 5.2.4. Summary of Marginal Effects for Logit Model on Union Membership:

- **Age:** A one-year increase in age is linked to a decrease in the likelihood of a worker being unionized by 0.05 percentage points. However, the effect is not statistically significant at the 5% level

- **College Graduation (collgrad):** Being a college graduate reduces the probability of being in a union by 0.88 percentage points. However, the effect is not statistically significant at the 5% level

- **Hours:** A one-unit increase in hours is associated with a 0.37 percentage point increase in the likelihood of a worker being unionized. 

- **Log of Wage (ln_wage):** A one-unit increase in the logarithm of wage is associated with a 20.76 percentage point increase in the likelihood of a worker being unionized. 



# 6 Assessment

<!-- Please put the R solution between 'begin solution' and 'end solution', but analyze the results outside the chunks -->

## Problem 1: Data Importing
Import the "card" dataset.

```{r,warning=FALSE}
card<-as.data.frame(read_excel("card.xlsx"))
```

## Problem 2: Drop the missing data

```{r,warning=FALSE}
#BEGIN SOLUTION

card_no_na <- na.omit(card)


#END SOLUTION
```

## Problem 3
Estimate a linear regression model that uses the log of the salary as the dependent variable and IQ, married, age, educ, and the log of weight as independent variables.

```{r,warning=FALSE}
#BEGIN SOLUTION


#END SOLUTION
```

## Problem 4: 
Which variables are the most important in the model? Explain

```{r,warning=FALSE}
#BEGIN SOLUTION


#END SOLUTION
```

## Problem 5: What can you conclude regarding homoscedasticity?

```{r,warning=FALSE}
#BEGIN SOLUTION


#END SOLUTION
```

## Problem 6: 

Interpret the coefficients of the variables

```{r,warning=FALSE}
#BEGIN SOLUTION

#END SOLUTION
```

## Problem 7: 

Create a binary variable that takes the value 1 if the salary is above the average and 0 otherwise

```{r,warning=FALSE}
average_wage <- mean(card_no_na$wage)

# Create a binary variable (high wage): 1 if wage is above average, 0 otherwise
card_no_na <- card_no_na %>%
  mutate(high_wage = ifelse(wage > average_wage, 1, 0))

```


## Problem 8: 
Estimate a logit model to explain the probability of an individual having a salary above the average, using the same independent variables as in the linear regression mode.

```{r,warning=FALSE}
#BEGIN SOLUTION

#END SOLUTION
```


## Problem 9:
Discuss how the independent variables are positively/negatively related to the probability of the salary being above the average.

```{r,warning=FALSE}
#BEGIN SOLUTION

#END SOLUTION
```

## Problem 10:
Compute the marginal effects of the logit model

```{r,warning=FALSE}
#BEGIN SOLUTION

#END SOLUTION
```



